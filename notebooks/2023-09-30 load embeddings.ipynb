{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict2 = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script from iteration 2.21 of ` 2023-09-30 save embeddings.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filepath, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, LocalFileStore(filepath), \n",
    "        namespace=f'{site_key}_{underlying_embeddings.model}'\n",
    "        )\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "    # return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filepath, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filepath,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "# filepath = '../embeddings/'\n",
    "# # doc_id = 1\n",
    "# # try:\n",
    "# #     directory = 'data'\n",
    "# #     doc_dict[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "# # except:\n",
    "# #     doc_dict[doc_id] = create_documents_from_csv()\n",
    "# #     print('Done creating doc from CSV')\n",
    "\n",
    "# doc_id = 2\n",
    "# try:\n",
    "#     directory = 'data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "# except:\n",
    "#     directory = '../data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "filepath = '../embeddings/'\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 load embeddings.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m \u001b[39m# filepath = '../embeddings/'\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39m# # doc_id = 1\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# # try:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m \u001b[39m#     directory = '../data'\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=146'>147</a>\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../embeddings/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m retriever_dict, description_dict \u001b[39m=\u001b[39m create_retriever_and_description_dicts(params_dict, filepath)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 load embeddings.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m description_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc_id \u001b[39min\u001b[39;00m params_dict:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     retriever_dict[doc_id] \u001b[39m=\u001b[39m create_retriever(\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         doc_dict[doc_id], params_dict[doc_id][\u001b[39m'\u001b[39m\u001b[39msite_key\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         filepath,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m         vector_dict\u001b[39m=\u001b[39mvector_dict, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m         text_splitter\u001b[39m=\u001b[39mparams_dict[doc_id]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext_splitter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     description_dict[params_dict[doc_id][\u001b[39m'\u001b[39m\u001b[39msite_key\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m params_dict[doc_id][\u001b[39m'\u001b[39m\u001b[39mdoc_description\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20load%20embeddings.ipynb#W6sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCreated retriever and description dicts for \u001b[39m\u001b[39m{\u001b[39;00mparams_dict\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filepath, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, LocalFileStore(filepath), \n",
    "        namespace=f'{site_key}_{underlying_embeddings.model}'\n",
    "        )\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    print(f'Retriever created for {site_key}')\n",
    "    return retriever_dict\n",
    "    # return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filepath, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in params_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filepath,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "    print(f'Created retriever and description dicts for {params_dict.keys()}')\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "# filepath = '../embeddings/'\n",
    "# # doc_id = 1\n",
    "# # try:\n",
    "# #     directory = 'data'\n",
    "# #     doc_dict[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "# # except:\n",
    "# #     doc_dict[doc_id] = create_documents_from_csv()\n",
    "# #     print('Done creating doc from CSV')\n",
    "\n",
    "# doc_id = 2\n",
    "# try:\n",
    "#     directory = 'data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "# except:\n",
    "#     directory = '../data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "filepath = '../embeddings/'\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1118.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n",
      "Retriever created for mattress\n",
      "Created retriever and description dicts for dict_keys([2])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filepath, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, LocalFileStore(filepath), \n",
    "        namespace=f'{site_key}_{underlying_embeddings.model}'\n",
    "        )\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    print(f'Retriever created for {site_key}')\n",
    "    return retriever_dict\n",
    "    # return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filepath, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filepath,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "    print(f'Created retriever and description dicts for {params_dict.keys()}')\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filepath = '../embeddings/'\n",
    "# doc_id = 1\n",
    "# try:\n",
    "#     directory = 'data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "# except:\n",
    "#     doc_dict[doc_id] = create_documents_from_csv()\n",
    "#     print('Done creating doc from CSV')\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mattress_text-embedding-ada-0024d698914-67ab-531b-a43a-655989f5061e']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(LocalFileStore(filepath).yield_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Recycle BC doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating doc from CSV\n",
      "Retriever created for recycle\n",
      "Created retriever and description dicts for dict_keys([1])\n"
     ]
    }
   ],
   "source": [
    "params_dict = {\n",
    "    1: {\n",
    "        'site_key': 'recycle',\n",
    "        'doc_description': recylebc,\n",
    "        'text_splitter': None\n",
    "    },\n",
    "    # 2: {\n",
    "    #     'site_key': 'mattress',\n",
    "    #     'doc_description': CoV_mattress,\n",
    "    #     'text_splitter': None\n",
    "    # }\n",
    "}\n",
    "filestore = '../embeddings/'\n",
    "doc_id = 1\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict2[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "except:\n",
    "    doc_dict2[doc_id] = create_documents_from_csv()\n",
    "    print('Done creating doc from CSV')\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore, doc_dict=doc_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
