{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original scripts from `chat_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(documents, site_key, vector_dict=vector_dict, text_splitter=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    embeddings_dict[site_key] = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: `create_retriever_and_description_dicts` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(documents, site_key, vector_dict=vector_dict, text_splitter=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    embeddings_dict[site_key] = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], parms_dict[doc_id]['site_key'], \n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=parms_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[parms_dict[doc_id]['site_key']] = parms_dict['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    1: {\n",
    "        'site_key': 'recycle',\n",
    "        'doc_description': recylebc,\n",
    "        'text_splitter': None\n",
    "    },\n",
    "    2: {\n",
    "        'site_key': 'matttress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params_dict[1].get('text_splitter', 'hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2 [Cache the embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/caching_embeddings#:~:text=Embeddings%20can%20be%20stored%20or%20temporarily%20cached%20to,is%20used%20as%20the%20key%20in%20the%20cache.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 246.56it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'mget'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     vector_dict[site_key] \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(texts, embeddings_dict[site_key])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:        \n",
      "\u001b[1;31mKeyError\u001b[0m: 'mattress'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     directory \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     doc_dict[doc_id] \u001b[39m=\u001b[39m create_documents(directory\u001b[39m=\u001b[39mdirectory, glob\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*.txt\u001b[39m\u001b[39m'\u001b[39m, loader_cls\u001b[39m=\u001b[39mTextLoader)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m retriever_dict, description_dict \u001b[39m=\u001b[39m create_retriever_and_description_dicts(params_dict, filestore)\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m description_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc_id \u001b[39min\u001b[39;00m doc_dict:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m     retriever_dict[doc_id] \u001b[39m=\u001b[39m create_retriever(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m         doc_dict[doc_id], params_dict[doc_id][\u001b[39m'\u001b[39m\u001b[39msite_key\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         filestore,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m         vector_dict\u001b[39m=\u001b[39mvector_dict, \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m         text_splitter\u001b[39m=\u001b[39mparams_dict[doc_id]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext_splitter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m         )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     description_dict[parms_dict[doc_id][\u001b[39m'\u001b[39m\u001b[39msite_key\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m parms_dict[\u001b[39m'\u001b[39m\u001b[39mdoc_description\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mreturn\u001b[39;00m retriever_dict, description_dict\n",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     underlying_embeddings \u001b[39m=\u001b[39m OpenAIEmbeddings(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         openai_organization\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mopenai_organization\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         openai_api_key\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mopenai_api_key\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     embeddings_dict[site_key] \u001b[39m=\u001b[39m CacheBackedEmbeddings\u001b[39m.\u001b[39mfrom_bytes_store(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         underlying_embeddings, filestore, namespace\u001b[39m=\u001b[39munderlying_embeddings\u001b[39m.\u001b[39mmodel\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m         )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     vector_dict[site_key] \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(texts, embeddings_dict[site_key])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m retriever_dict[site_key] \u001b[39m=\u001b[39m vector_dict[site_key]\u001b[39m.\u001b[39mas_retriever()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#W4sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mreturn\u001b[39;00m retriever_dict\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\vectorstores\\base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    416\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\vectorstores\\faiss.py:602\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    577\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    583\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m    584\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[0;32m    604\u001b[0m         texts,\n\u001b[0;32m    605\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    610\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\embeddings\\cache.py:109\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_documents\u001b[39m(\u001b[39mself\u001b[39m, texts: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[List[\u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Embed a list of texts.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m    The method first checks the cache for the embeddings.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39m        A list of embeddings for the given texts.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     vectors: List[Union[List[\u001b[39mfloat\u001b[39m], \u001b[39mNone\u001b[39;00m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embedding_store\u001b[39m.\u001b[39mmget(\n\u001b[0;32m    110\u001b[0m         texts\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    112\u001b[0m     missing_indices: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[0;32m    113\u001b[0m         i \u001b[39mfor\u001b[39;00m i, vector \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vectors) \u001b[39mif\u001b[39;00m vector \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     ]\n\u001b[0;32m    115\u001b[0m     missing_texts \u001b[39m=\u001b[39m [texts[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m missing_indices]\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:70\u001b[0m, in \u001b[0;36mEncoderBackedStore.mget\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m encoded_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_encoder(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys]\n\u001b[1;32m---> 70\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_deserializer(value) \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values\n\u001b[0;32m     74\u001b[0m ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'mget'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filestore, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    try:    \n",
    "        vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    except KeyError:        \n",
    "        underlying_embeddings = OpenAIEmbeddings(\n",
    "            openai_organization=os.environ['openai_organization'],\n",
    "            openai_api_key=os.environ['openai_api_key']\n",
    "            )\n",
    "        embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings, filestore, namespace=underlying_embeddings.model\n",
    "            )\n",
    "        vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filestore, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filestore,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[parms_dict[doc_id]['site_key']] = parms_dict['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filestore = '../embeddings/'\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\nWhen you recycle your mattress (foam or futon) and box spring, you reduce the amount of waste going to our landfill.\\nTo recycle your mattress and box spring, you can:\\n\\nBring it to the \\xa0transfer station or landfill\\xa0for a fee\\nHire\\xa0a mattress recycling company or junk removal company\\n\\nImportant note Don't dump your mattress in an alley or on the street.\\xa0It's illegal and you could be fined for illegal dumping.\\nView the fee, limits, and locations to recycle a\\xa0mattress, futon, and box spring\\nhttps://vancouver.ca/home-property-development/how-to-dispose-of-mattresses.aspx\", metadata={'source': '..\\\\data\\\\mattress.txt'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 901.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filestore, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, filestore, namespace=underlying_embeddings.model\n",
    "        )\n",
    "    # vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    # retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filestore, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filestore,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filestore = '../embeddings/'\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(params_dict[doc_id].get('text_splitter', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 497.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filestore, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, filestore, namespace=underlying_embeddings.model\n",
    "        )\n",
    "    # vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    # retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    # return retriever_dict\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filestore, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filestore,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filestore = '../embeddings/'\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.embeddings.cache.CacheBackedEmbeddings at 0x250d67353d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_dict[2]['mattress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document_embedding_store', 'underlying_embeddings'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(retriever_dict[2]['mattress']).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CacheBackedEmbeddings' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mgetattr\u001b[39m(retriever_dict[\u001b[39m2\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmattress\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CacheBackedEmbeddings' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "getattr(retriever_dict[2]['mattress'], 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.storage.encoder_backed.EncoderBackedStore at 0x250d67de090>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(retriever_dict[2]['mattress'], 'document_embedding_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.storage.encoder_backed.EncoderBackedStore"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(getattr(retriever_dict[2]['mattress'], 'document_embedding_store'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mattress': <langchain.embeddings.cache.CacheBackedEmbeddings at 0x250d67353d0>}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_dict[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'mget'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\silvh\\OneDrive\\lighthouse\\portfolio-projects\\datajam-chatbot\\chatbot_tuning\\notebooks\\2023-09-30 save embeddings.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m site_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmattress\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/silvh/OneDrive/lighthouse/portfolio-projects/datajam-chatbot/chatbot_tuning/notebooks/2023-09-30%20save%20embeddings.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vector_dict[site_key] \u001b[39m=\u001b[39m FAISS\u001b[39m.\u001b[39mfrom_documents(doc_dict[doc_id], retriever_dict[\u001b[39m2\u001b[39m][site_key])\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\vectorstores\\base.py:417\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    416\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\vectorstores\\faiss.py:602\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    577\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    583\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m FAISS:\n\u001b[0;32m    584\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \n\u001b[0;32m    586\u001b[0m \u001b[39m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[39m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m     embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts)\n\u001b[0;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m__from(\n\u001b[0;32m    604\u001b[0m         texts,\n\u001b[0;32m    605\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    610\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\embeddings\\cache.py:109\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_documents\u001b[39m(\u001b[39mself\u001b[39m, texts: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[List[\u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Embed a list of texts.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m    The method first checks the cache for the embeddings.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39m        A list of embeddings for the given texts.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     vectors: List[Union[List[\u001b[39mfloat\u001b[39m], \u001b[39mNone\u001b[39;00m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_embedding_store\u001b[39m.\u001b[39mmget(\n\u001b[0;32m    110\u001b[0m         texts\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    112\u001b[0m     missing_indices: List[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[0;32m    113\u001b[0m         i \u001b[39mfor\u001b[39;00m i, vector \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vectors) \u001b[39mif\u001b[39;00m vector \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     ]\n\u001b[0;32m    115\u001b[0m     missing_texts \u001b[39m=\u001b[39m [texts[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m missing_indices]\n",
      "File \u001b[1;32mc:\\Users\\silvh\\.conda\\envs\\datajam\\Lib\\site-packages\\langchain\\storage\\encoder_backed.py:70\u001b[0m, in \u001b[0;36mEncoderBackedStore.mget\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get the values associated with the given keys.\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m encoded_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_encoder(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys]\n\u001b[1;32m---> 70\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore\u001b[39m.\u001b[39mmget(encoded_keys)\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_deserializer(value) \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m value\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values\n\u001b[0;32m     74\u001b[0m ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'mget'"
     ]
    }
   ],
   "source": [
    "site_key = 'mattress'\n",
    "vector_dict[site_key] = FAISS.from_documents(doc_dict[doc_id], retriever_dict[2][site_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.21 Successfully saved embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 116.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filestore, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, filestore, \n",
    "        namespace=f'{site_key}_{underlying_embeddings.model}'\n",
    "        )\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "    # return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filepath, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            LocalFileStore(filepath),\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filestore = '../embeddings/'\n",
    "# doc_id = 1\n",
    "# try:\n",
    "#     directory = 'data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "# except:\n",
    "#     doc_dict[doc_id] = create_documents_from_csv()\n",
    "#     print('Done creating doc from CSV')\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mattress': <langchain.embeddings.cache.CacheBackedEmbeddings at 0x20fe9689150>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mattress_text-embedding-ada-0024d698914-67ab-531b-a43a-655989f5061e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '../embeddings/'\n",
    "list(LocalFileStore(filepath).yield_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(\n",
    "    documents, site_key, filepath, \n",
    "    embeddings_dict=embeddings_dict, \n",
    "    vector_dict=vector_dict, text_splitter=None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "   \n",
    "    underlying_embeddings = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    embeddings_dict[site_key] = CacheBackedEmbeddings.from_bytes_store(\n",
    "        underlying_embeddings, LocalFileStore(filepath), \n",
    "        namespace=f'{site_key}_{underlying_embeddings.model}'\n",
    "        )\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "    # return embeddings_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, filepath, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], params_dict[doc_id]['site_key'], \n",
    "            filepath,\n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=params_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[params_dict[doc_id]['site_key']] = params_dict[doc_id]['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    # 1: {\n",
    "    #     'site_key': 'recycle',\n",
    "    #     'doc_description': recylebc,\n",
    "    #     'text_splitter': None\n",
    "    # },\n",
    "    2: {\n",
    "        'site_key': 'mattress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "filepath = '../embeddings/'\n",
    "# doc_id = 1\n",
    "# try:\n",
    "#     directory = 'data'\n",
    "#     doc_dict[doc_id] = create_documents(directory=directory, glob='*.csv')\n",
    "# except:\n",
    "#     doc_dict[doc_id] = create_documents_from_csv()\n",
    "#     print('Done creating doc from CSV')\n",
    "\n",
    "doc_id = 2\n",
    "try:\n",
    "    directory = 'data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "except:\n",
    "    directory = '../data'\n",
    "    doc_dict[doc_id] = create_documents(directory=directory, glob='*.txt', loader_cls=TextLoader)\n",
    "\n",
    "\n",
    "\n",
    "retriever_dict, description_dict = create_retriever_and_description_dicts(params_dict, filestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
