{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "[]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original scripts from `chat_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(documents, site_key, vector_dict=vector_dict, text_splitter=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    embeddings_dict[site_key] = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "\n",
    "# Creating the Agent\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Create memory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import StreamlitChatMessageHistory # for Streamlit\n",
    "\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain.schema.messages import SystemMessage\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.openai_functions_agent.agent_token_buffer_memory import AgentTokenBufferMemory\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "directory='../data/' # This is the directory containing the CSV/text files.\n",
    "\n",
    "# Initialize Dictionaries\n",
    "tool_dict = dict()\n",
    "embeddings_dict = dict()\n",
    "db_dict = dict()\n",
    "retriever_dict = dict()\n",
    "vector_dict = dict()\n",
    "description_dict = dict()\n",
    "answer_dict=dict()\n",
    "conversation_dict = dict()\n",
    "doc_dict = dict()\n",
    "queries_dict = dict()\n",
    "\n",
    "def create_documents(directory='../data', glob='**/[!.]*', show_progress=True, loader_cls=CSVLoader):\n",
    "    loader = DirectoryLoader(\n",
    "        directory, glob=glob, show_progress=show_progress,\n",
    "        loader_cls=loader_cls)\n",
    "\n",
    "    documents = loader.load()\n",
    "    print(f'Number of files: {len(documents)}')\n",
    "    return documents\n",
    "    \n",
    "def create_documents_from_csv(file_path='../data/Datajam_2023___Fine_Tuning_ChatBot_CSV_-_Recycle_BC_1.csv'):\n",
    "    loader = CSVLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def create_retriever(documents, site_key, vector_dict=vector_dict, text_splitter=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        - text_splitter (optional): a text splitter object. If None, the documents are not split. \n",
    "    \"\"\"\n",
    "    embeddings_dict[site_key] = OpenAIEmbeddings(\n",
    "        openai_organization=os.environ['openai_organization'],\n",
    "        openai_api_key=os.environ['openai_api_key']\n",
    "        )\n",
    "    if text_splitter is None: # object type is the same (class 'langchain.schema.document.Document') whether or not the documents are split\n",
    "        texts = documents\n",
    "    else:\n",
    "        texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    vector_dict[site_key] = FAISS.from_documents(texts, embeddings_dict[site_key])\n",
    "    retriever_dict[site_key] = vector_dict[site_key].as_retriever()\n",
    "    return retriever_dict\n",
    "\n",
    "\n",
    "def create_retriever_and_description_dicts(params_dict, doc_dict=doc_dict, vector_dict=vector_dict):\n",
    "    retriever_dict = dict()\n",
    "    description_dict = dict()\n",
    "    for doc_id in doc_dict:\n",
    "        retriever_dict[doc_id] = create_retriever(\n",
    "            doc_dict[doc_id], parms_dict[doc_id]['site_key'], \n",
    "            vector_dict=vector_dict, \n",
    "            text_splitter=parms_dict[doc_id].get('text_splitter', None)\n",
    "            )\n",
    "        description_dict[parms_dict[doc_id]['site_key']] = parms_dict['doc_description']\n",
    "\n",
    "    return retriever_dict, description_dict\n",
    "\n",
    "def create_tools_list(retriever_dict, description_dict):\n",
    "    \"\"\"\n",
    "    https://api.python.langchain.com/en/latest/agents/langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool.html?highlight=create_retriever_tool#langchain.agents.agent_toolkits.conversational_retrieval.tool.create_retriever_tool\n",
    "    \"\"\"\n",
    "    tools_list = []\n",
    "    for site_key, retriever in retriever_dict.items():\n",
    "        tool_name = f'search_{site_key}'\n",
    "        tool = create_retriever_tool(retriever_dict[site_key], tool_name, description_dict[site_key])\n",
    "        tools_list.append(tool)\n",
    "    return tools_list\n",
    "\n",
    "\n",
    "recylebc = \"\"\"\n",
    "This document provides information from the Recycle BC website or BC government \n",
    "website. It has the most specific information \n",
    "about whether or not an item is accepted for recycling and where to recycle it.\n",
    "This should be the main resource for recycling information for residents of British Columbia.\n",
    "\"\"\"\n",
    "\n",
    "CoV_mattress = \"\"\"\n",
    "Information from the City of Vancouver website about how to recycle mattresses.\n",
    "\"\"\"\n",
    "\n",
    "params_dict = {\n",
    "    1: {\n",
    "        'site_key': 'recycle',\n",
    "        'doc_description': recylebc,\n",
    "        'text_splitter': None\n",
    "    },\n",
    "    2: {\n",
    "        'site_key': 'matttress',\n",
    "        'doc_description': CoV_mattress,\n",
    "        'text_splitter': None\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params_dict[1].get('text_splitter', 'hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *End of Page*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
